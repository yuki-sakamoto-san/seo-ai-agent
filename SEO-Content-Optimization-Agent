import os
import re
import json
import argparse
import logging
from typing import List, Dict, Any, Optional

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

from dotenv import load_dotenv

# --- Gemini ---
import google.generativeai as genai

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# ----------------------
# Utilities
# ----------------------
def read_env():
    load_dotenv()
    gemini_key = os.getenv("GEMINI_API_KEY")
    if not gemini_key:
        raise RuntimeError("GEMINI_API_KEY is missing. Set it in your environment or .env")
    genai.configure(api_key=gemini_key)
    return {
        "gemini_key": gemini_key,
        "serpapi_key": os.getenv("SERPAPI_API_KEY"),
        "google_key": os.getenv("GOOGLE_API_KEY"),
        "google_cse_id": os.getenv("GOOGLE_CSE_ID"),
    }

def normalize_ws(text: str) -> str:
    return re.sub(r"\s+", " ", text).strip()

# ----------------------
# Crawling & Extraction
# ----------------------
def fetch_html(url: str, timeout: int = 20) -> str:
    headers = {"User-Agent": "Mozilla/5.0 (compatible; GeminiSEOAgent/1.0)"}
    resp = requests.get(url, headers=headers, timeout=timeout)
    resp.raise_for_status()
    return resp.text

def extract_json_ld(soup: BeautifulSoup) -> List[Dict[str, Any]]:
    items = []
    for tag in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            data = json.loads(tag.string or "{}")
            if isinstance(data, list):
                items.extend(data)
            elif isinstance(data, dict):
                items.append(data)
        except Exception:
            continue
    return items

def extract_headings(soup: BeautifulSoup) -> List[Dict[str, str]]:
    out = []
    for hn in ["h1", "h2", "h3", "h4", "h5", "h6"]:
        for h in soup.find_all(hn):
            out.append({"tag": hn, "text": normalize_ws(h.get_text(" ", strip=True))})
    return out

def extract_images(soup: BeautifulSoup, base_url: str) -> List[Dict[str, str]]:
    imgs = []
    for img in soup.find_all("img"):
        src = img.get("src") or ""
        alt = img.get("alt") or ""
        if src:
            full = urljoin(base_url, src)
            imgs.append({"src": full, "alt": alt})
    return imgs

def extract_meta(soup: BeautifulSoup) -> Dict[str, Any]:
    title = soup.title.string.strip() if soup.title and soup.title.string else ""
    metas = {m.get("name") or m.get("property"): m.get("content") for m in soup.find_all("meta") if m.get("content")}
    return {"title": title, "meta": metas}

def extract_main_text(soup: BeautifulSoup) -> str:
    for bad in soup(["script", "style", "nav", "footer", "noscript"]):
        bad.decompose()
    texts = []
    for tag in soup.find_all(["p", "li", "blockquote"]):
        t = normalize_ws(tag.get_text(" ", strip=True))
        if t and len(t) > 30:
            texts.append(t)
    return "\n".join(texts)

def crawl_page(url: str) -> Dict[str, Any]:
    html = fetch_html(url)
    soup = BeautifulSoup(html, "lxml")
    return {
        "url": url,
        "meta": extract_meta(soup),
        "headings": extract_headings(soup),
        "images": extract_images(soup, url),
        "json_ld": extract_json_ld(soup),
        "text": extract_main_text(soup),
    }

# ----------------------
# SERP Providers (stub + extendable)
# ----------------------
def serp_stub(query: str, locale: str = "ja-JP", num: int = 5) -> List[Dict[str, Any]]:
    logging.warning("SERP provider is in STUB mode. Plug in SerpAPI/Google CSE for real results.")
    return [
        {"url": "https://example.com/competitor-1", "title": f"{query} とは？完全ガイド", "snippet": "定義・メリット・導入手順"},
        {"url": "https://example.com/competitor-2", "title": f"{query} の基礎", "snippet": "初心者向けにわかりやすく解説"},
    ]

def collect_serp(query: str, keys: Dict[str, Optional[str]], locale: str, num: int = 8) -> List[Dict[str, Any]]:
    # TODO: replace with SerpAPI/Google CSE if you add keys in .env
    return serp_stub(query, locale, num)

# ----------------------
# Gemini prompts
# ----------------------
SYSTEM_PROMPT = """You are an SEO Optimization Agent.
You evaluate a source page against a primary query and top SERP competitors.
Produce specific, prioritized, and implementable recommendations.
Consider Japanese and global best practices, featured snippets, PAA, schema, media, E-E-A-T, and intent match.
Output JSON only. Do not include commentary outside JSON.
"""

def build_user_prompt(page: Dict[str, Any], serp_pages: List[Dict[str, Any]], primary_query: str, locale: str) -> str:
    page_text = page.get("text", "")
    if len(page_text) > 15000:
        page_text = page_text[:15000] + " ...[truncated]"
    serp_summary = [{"rank": i+1, "url": s.get("url"), "title": s.get("title"), "snippet": s.get("snippet")} for i, s in enumerate(serp_pages)]
    return json.dumps({
        "primary_query": primary_query,
        "locale": locale,
        "page": {
            "url": page.get("url"),
            "meta": page.get("meta"),
            "headings": page.get("headings")[:100],
            "images": page.get("images")[:50],
            "json_ld": page.get("json_ld")[:10],
            "text_excerpt": page_text,
        },
        "serp_competitors": serp_summary
    }, ensure_ascii=False)

def ensure_model(model_name: str = "gemini-1.5-pro") -> genai.GenerativeModel:
    return genai.GenerativeModel(model_name)

def call_gemini_recommendations(model, system_prompt: str, user_payload: str) -> Dict[str, Any]:
    response = model.generate_content([
        {"role": "system", "parts": [system_prompt]},
        {"role": "user", "parts": [user_payload]},
    ])
    text = response.text.strip()
    try:
        return json.loads(text)
    except Exception:
        m = re.search(r"\{.*\}", text, flags=re.S)
        if m:
            return json.loads(m.group(0))
        raise

# ----------------------
# Main pipeline
# ----------------------
def run(url: str, primary_query: str, locale: str = "ja-JP", max_serp: int = 8, model_name: str = "gemini-1.5-pro") -> Dict[str, Any]:
    keys = read_env()
    logging.info("Crawling page...")
    page = crawl_page(url)
    logging.info("Collecting SERP...")
    serp = collect_serp(primary_query, keys, locale, num=max_serp)
    logging.info("Asking Gemini for recommendations...")
    model = ensure_model(model_name)
    user_payload = build_user_prompt(page, serp, primary_query, locale)
    recs = call_gemini_recommendations(model, SYSTEM_PROMPT, user_payload)
    return {
        "input": {"url": url, "primary_query": primary_query, "locale": locale},
        "serp_sample": serp,
        "recommendations": recs,
    }

def save_output(data: Dict[str, Any], outdir: str = "./output") -> str:
    os.makedirs(outdir, exist_ok=True)
    path = os.path.join(outdir, "recommendations.json")
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    return path

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Gemini SEO Optimization Agent")
    parser.add_argument("--url", required=True, help="URL of the page to optimize")
    parser.add_argument("--query", required=True, help="Primary query to rank for")
    parser.add_argument("--locale", default="ja-JP", help="Locale like ja-JP or en-US")
    parser.add_argument("--max-serp", type=int, default=8)
    parser.add_argument("--model", default="gemini-1.5-pro")
    args = parser.parse_args()

    try:
        result = run(args.url, args.query, args.locale, args.max_serp, args.model)
        print(json.dumps(result, ensure_ascii=False, indent=2))
        out_path = save_output(result)
        logging.info(f"Wrote {out_path}")
    except Exception as e:
        logging.exception(e)
        exit(1)
